{"cells":[{"cell_type":"markdown","metadata":{"id":"X1BtiuAOfHfX"},"source":["## Install"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":108655,"status":"ok","timestamp":1702018253452,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"},"user_tz":-420},"id":"xPhy5XqEd_KU"},"outputs":[],"source":["from IPython.display import clear_output\n","#install\n","!pip install -U transformers\n","!pip install -U huggingface-hub\n","!pip install -U langchain\n","!pip install -U accelerate\n","!pip install -U datasets\n","\n","# not exit in python 3.11\n","#!pip install -U intel_extension_for_pytorch\n","\n","!pip install -U bitsandbytes\n","# use in windows machine. !!!May not be safe!!!\n","#!pip install -U bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n","!pip install -U peft\n","!pip install -U trl\n","!pip install -U pprintpp\n","\n","\n","# install FlashAttention 2\n","# !pip install -U ninja\n","# !pip install -U packaging\n","# !MAX_JOBS=4 pip install -U flash-attn --no-build-isolation # if error -> use pip install -U flash-attn\n","\n","!pip install -U streamlit\n","!pip install -U langchain-experimental\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"6C_lztdCQST6"},"source":["## Colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3268,"status":"ok","timestamp":1702018256711,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"},"user_tz":-420},"id":"J79KlPRYQEXH","outputId":"3543f940-3188-4340-869a-3defaf1d6128"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1702018256712,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"},"user_tz":-420},"id":"TCNNlfzyQRq_","outputId":"1c13c003-393f-4b2d-9748-7434619189d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/zephyr\n"]}],"source":["%cd \"/content/drive/MyDrive/Colab Notebooks/zephyr\""]},{"cell_type":"markdown","metadata":{"id":"CXAps0JlfPBr"},"source":["## Import"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":341,"status":"ok","timestamp":1702021716805,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"},"user_tz":-420},"id":"w6YGVVK8uAWZ"},"outputs":[],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    Conversation,\n",")\n","from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n","from langchain import PromptTemplate, LLMChain\n","import numpy as np\n","import pandas as pd\n","import torch\n","from datasets import Dataset, load_dataset\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","from sklearn.model_selection import train_test_split\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","cache_dir=\"./Model/\"\n","model_name=\"HuggingFaceH4/zephyr-7b-beta\"\n","\n","import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n"]},{"cell_type":"markdown","metadata":{"id":"ogSZ1QMLMk1g"},"source":["## Model"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"4WU9a0FTpMxv","executionInfo":{"status":"ok","timestamp":1702021718582,"user_tz":-420,"elapsed":479,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"}}},"outputs":[],"source":["class Chatbot():\n","  def __init__(self,model_name = \"HuggingFaceH4/zephyr-7b-beta\",cache_dir=\"./Model/\",max_vram = 7):\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    self.model_name = model_name\n","    self.cache_dir = cache_dir\n","    self.quantization_config = BitsAndBytesConfig( load_in_4bit=True,\n","                                                  bnb_4bit_compute_dtype=torch.bfloat16,\n","                                                  bnb_4bit_use_double_quant=True,\n","                                                  bnb_4bit_quant_type='nf4')\n","    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,cache_dir=self.cache_dir)\n","    self.tokenizer.pad_token = self.tokenizer.eos_token\n","    self.tokenizer.padding_side = \"right\"\n","    self.pipe=None\n","    self.langchain_hf=None\n","    self.max_vram = max_vram\n","\n","  def InitChat(self,lora=None):\n","    torch.cuda.empty_cache()\n","    self.model = AutoModelForCausalLM.from_pretrained( model_name,\n","                                                      cache_dir=self.cache_dir,\n","                                                      low_cpu_mem_usage=True,\n","                                                      return_dict=True,\n","                                                      quantization_config=self.quantization_config,\n","                                                      # if error -> use use_flash_attention_1\n","                                                      #use_flash_attention_2=True,\n","                                                      max_memory={0: f\"{self.max_vram}GB\"},\n","                                                      device_map=\"auto\",\n","                                                      torch_dtype=torch.bfloat16,\n","      )\n","    if lora != None:\n","      self.model = PeftModel.from_pretrained(self.model, lora)\n","      self.model = self.model.merge_and_unload()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer)\n","\n","  def InitLangChain(self,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer,\n","                         do_sample=True,max_new_tokens = max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p)\n","    self.langchain_hf = HuggingFacePipeline(pipeline=self.pipe)\n","  def CreatConversation(self):\n","    return  [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"You are a wonderful and trustworthy assistant. If you receive a request in Vietnamese, please respond in Vietnamese.\",\n","        }\n","    ]\n","  def __AddUserInput(self,content,messages):\n","     messages.append({\"role\": \"user\", \"content\": f\"{content.strip()}\"})\n","     return messages\n","\n","  def __AddAssistantOutput(self,content,messages):\n","    temp = (content.split(\"<|assistant|>\"))[-1].strip()\n","    messages.append({\"role\": \"assistant\", \"content\": f\"{temp}\"})\n","    return messages\n","\n","  def Chat(self,content:str =\"\",messages = None,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95):\n","    torch.cuda.empty_cache()\n","    if len(content)==0:\n","      return None\n","    if messages == None:\n","      messages = self.CreatConversation()\n","\n","    messages = self.__AddUserInput(content,messages)\n","    return self._Generate(messages,max_new_tokens,temperature, top_k, top_p)\n","\n","  def _Generate(self,messages,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    # use_flash_attention_1 if use_flash_attention_2 error\n","    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n","      prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","      outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p)\n","      messages = self.__AddAssistantOutput(outputs[0][\"generated_text\"],messages)\n","    return messages\n","\n","  def GetLastContent(self,messages):\n","    return (messages[-1][\"content\"])"]},{"cell_type":"markdown","metadata":{"id":"RVjF_bP_fsp3"},"source":["## Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["a3c9cdf96f244523afa15a7c20ba58f1"]},"id":"rJ0YSvTBgK0e","outputId":"37e3eceb-7935-4329-f49d-f56223aa6b64"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3c9cdf96f244523afa15a7c20ba58f1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["chat = Chatbot()\n","chat.InitChat()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10520,"status":"ok","timestamp":1701676140142,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"xUu-rhOSaCRA","outputId":"67d7859b-560c-4810-a784-893eccbc2e7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Xin chÃ o, tÃ´i cÃ¡ nháº­n ráº¥t vui vá»›i viá»‡c giÃºp báº¡n. Vui lÃ²ng Ä‘á»ƒ láº¡i yÃªu cáº§u cá»§a báº¡n á»Ÿ Ä‘Ã¢y, tÃ´i sáº½ tráº£ lá»i ngay.\n"]}],"source":["messages = chat.Chat(\"Xin chÃ o tÃ´i cáº§n giÃºp Ä‘á»¡\",max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTdkGqAC2K7t","outputId":"f4e067c3-4284-4488-eecd-76d081f2e034"},"outputs":[{"name":"stdout","output_type":"stream","text":["Má»™t nÄƒm cÃ³ 365 ngÃ y (trá»« nÄƒm nhuáº­n Ä‘Æ¡n) trong khoáº£ng giá»¯a hai ngÃ y cá»§a thÃ¡ng 12 vÃ  thÃ¡ng 0 (cÃ¡c mÃ´i trÆ°á»ng khÃ¡c cÃ³ thá»ƒ sá»­ dá»¥ng há»‡ thá»‘ng nÄƒm khÃ¡c nhÆ° 360 ngÃ y trong má»™t nÄƒm).\n"]}],"source":["messages = chat.Chat(\"Má»™t nÄƒm cÃ³ bao nhiÃªu ngÃ y\",messages,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33562,"status":"ok","timestamp":1701676179893,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"hHX2DuUja7um","outputId":"4cf8dc54-b56f-41a1-b460-a3c8a66dc4f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Äá»™c quyá»n (copyright) lÃ  quyá»n tÃ¡c giáº£ hoáº·c tÃ¡c giáº£ cÃ³ quyá»n sá»Ÿ há»¯u Ä‘Æ°á»£c Ä‘Æ°a ra khi táº¡o ra má»™t táº­p lÆ°á»£c, hÃ¬nh áº£nh, khÃ¡m phÃ¡, Ä‘á»™ng thÃ¡i, vÄƒn báº£n, hoáº·c khÃ´ng gian báº£n quyá»n khÃ¡c. NÃ³ cho phÃ©p chá»§ sá»Ÿ há»¯u cÃ³ quyá»n tá»± do Ä‘á»ƒ chá»n cÃ¡c cÃ¡ch sá»­ dá»¥ng, phÃ¢n phá»‘i, sáº£n xuáº¥t, hoáº·c bÃ¡n cá»§a báº£n quyá»n Ä‘Ã³. Äá»™c quyá»n cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c truyá»n qua khÃ´ng gian báº£n quyá»n sau khi chá»§ sá»Ÿ há»¯u cháº¿t, nhÆ°ng thá»i gian nÃ  cÃ³ thá»ƒ khÃ¡c tÃ¹y thá»ƒ sá»± nghiá»‡p cá»§a chá»§ sá»Ÿ há»¯u.\n"]}],"source":["messages = chat.Chat(\"Gáº£i thÃ­ch thuáº­t ngá»¯ \\\"Ä‘á»™c quyá»n\\\"\",messages,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11708,"status":"ok","timestamp":1701676191598,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"ZgypQ26BbGaa","outputId":"41498ca2-4b14-4d11-ccbb-3d2ac88b0895"},"outputs":[{"name":"stdout","output_type":"stream","text":["Xin hÃ£y khÃ´ng heesitate náº¿u cÃ³ báº¥t ká»³ tháº¯c máº¯c nÃ  khÃ¡c, tÃ´i sáº½ cá»‘ gáº¯ng cung cáº¥p cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c nháº¥t cÃ³ sáºµn.\n"]}],"source":["messages = chat.Chat(\"Cáº£m Æ¡n thÃ´ng tin ráº¥t há»¯u Ã­ch\",messages,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1701676191598,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"re4gHEvqsn3a","outputId":"cf53920c-bf09-4845-e793-09e94159f372"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'role': 'system', 'content': 'You are a wonderful and trustworthy assistant. If you receive a request in Vietnamese, please respond in Vietnamese.'}, {'role': 'user', 'content': 'Xin chÃ o tÃ´i cáº§n giÃºp Ä‘á»¡'}, {'role': 'assistant', 'content': 'Xin chÃ o, tÃ´i cÃ¡ nháº­n ráº¥t vui vá»›i viá»‡c giÃºp báº¡n. Vui lÃ²ng Ä‘á»ƒ láº¡i yÃªu cáº§u cá»§a báº¡n á»Ÿ Ä‘Ã¢y, tÃ´i sáº½ tráº£ lá»i ngay.'}, {'role': 'user', 'content': 'Má»™t nÄƒm cÃ³ bao nhiÃªu ngÃ y'}, {'role': 'assistant', 'content': 'Má»™t nÄƒm cÃ³ 365 ngÃ y (trá»« nÄƒm nhuáº­n Ä‘Æ¡n) trong khoáº£ng giá»¯a hai ngÃ y cá»§a thÃ¡ng 12 vÃ  thÃ¡ng 0 (cÃ¡c mÃ´i trÆ°á»ng khÃ¡c cÃ³ thá»ƒ sá»­ dá»¥ng há»‡ thá»‘ng nÄƒm khÃ¡c nhÆ° 360 ngÃ y trong má»™t nÄƒm).'}, {'role': 'user', 'content': 'Gáº£i thÃ­ch thuáº­t ngá»¯ \"Ä‘á»™c quyá»n\"'}, {'role': 'assistant', 'content': 'Äá»™c quyá»n (copyright) lÃ  quyá»n tÃ¡c giáº£ hoáº·c tÃ¡c giáº£ cÃ³ quyá»n sá»Ÿ há»¯u Ä‘Æ°á»£c Ä‘Æ°a ra khi táº¡o ra má»™t táº­p lÆ°á»£c, hÃ¬nh áº£nh, khÃ¡m phÃ¡, Ä‘á»™ng thÃ¡i, vÄƒn báº£n, hoáº·c khÃ´ng gian báº£n quyá»n khÃ¡c. NÃ³ cho phÃ©p chá»§ sá»Ÿ há»¯u cÃ³ quyá»n tá»± do Ä‘á»ƒ chá»n cÃ¡c cÃ¡ch sá»­ dá»¥ng, phÃ¢n phá»‘i, sáº£n xuáº¥t, hoáº·c bÃ¡n cá»§a báº£n quyá»n Ä‘Ã³. Äá»™c quyá»n cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c truyá»n qua khÃ´ng gian báº£n quyá»n sau khi chá»§ sá»Ÿ há»¯u cháº¿t, nhÆ°ng thá»i gian nÃ  cÃ³ thá»ƒ khÃ¡c tÃ¹y thá»ƒ sá»± nghiá»‡p cá»§a chá»§ sá»Ÿ há»¯u.'}, {'role': 'user', 'content': 'Cáº£m Æ¡n thÃ´ng tin ráº¥t há»¯u Ã­ch'}, {'role': 'assistant', 'content': 'Xin hÃ£y khÃ´ng heesitate náº¿u cÃ³ báº¥t ká»³ tháº¯c máº¯c nÃ  khÃ¡c, tÃ´i sáº½ cá»‘ gáº¯ng cung cáº¥p cÃ¡c thÃ´ng tin chÃ­nh xÃ¡c nháº¥t cÃ³ sáºµn.'}]\n"]}],"source":["print(messages)"]},{"cell_type":"markdown","metadata":{"id":"YPum52jHOKQN"},"source":["### Using LangChain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4e0469bec19a4932bd326f097fdc4877","6eeb77a98251458b939fdf77a51f6fb6","530f8864945d4c629f8c79c532976904","49c2095c4d5046a9aef6f4bc0731fd68","7bcde2193c794db2956d14d30eb9a1a4","fac6e0af92824eb7b30596ab3eac24a0","3e4ce89f991b44e284177d24c8514753","bb1934c7bb564828a63203119b5a7572","d8c44e79eb954271bc1f89f3a64fab94","fc815aea32644f40a4bf4c4155d82b09","79a81d43658c4e7f8842233526fedfca"]},"executionInfo":{"elapsed":357436,"status":"ok","timestamp":1701933943156,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"},"user_tz":-420},"id":"imzmu9VpONQh","outputId":"73bd7c88-649f-494b-cff4-e9c6c7d42dda"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0469bec19a4932bd326f097fdc4877"}},"metadata":{}}],"source":["chat = Chatbot(max_vram=13)\n","chat.InitLangChain(max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95)"]},{"cell_type":"markdown","source":["### Pandas"],"metadata":{"id":"S5fXnzIPOyFa"}},{"cell_type":"code","source":["from langchain.agents.agent_types import AgentType\n","from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n","import pandas as pd\n","\n","\n","data = pd.read_excel(\"Data.xlsx\")\n","\n","agent = create_pandas_dataframe_agent(chat.langchain_hf, data, verbose=True)\n","\n","agent.run(\"CÃ³ bao nhiÃªu ngÃ nh há»c á»Ÿ trÆ°á»ng khoa há»c mÃ¡y tÃ­nh\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"xN9bI6tZLQaC","executionInfo":{"status":"ok","timestamp":1701935711640,"user_tz":-420,"elapsed":737421,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"}},"outputId":"0262ed4e-03c7-482c-abc4-3c9af14ce8d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32;1m\u001b[1;3mThought: TÃ¬m kiáº¿m tá»« khoa há»c mÃ¡y tÃ­nh trong báº£ng\n","Action: python_repl_ast\n","Action Input: len(df[(df['content'].str.contains('Khoa há»c mÃ¡y tÃ­nh'))])\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m1\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m ÄÆ°á»£c biáº¿t cÃ³ 1 ngÃ nh há»c á»Ÿ trÆ°á»ng khoa há»c mÃ¡y tÃ­nh\n","Final Answer: 1\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"H_C_qPBS7sZI"},"source":["## Streamlit"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4595,"status":"ok","timestamp":1702021743212,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"},"user_tz":-420},"id":"PRMxqtt-82ms","outputId":"aac433ef-dfa9-4aaa-a129-b10c6ab9b09b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/drive/MyDrive/Colab Notebooks/zephyr/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/drive/MyDrive/Colab Notebooks/zephyr/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","updated 1 package and audited 36 packages in 3.561s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n","  run `npm audit fix` to fix them, or `npm audit` for details\n","\u001b[K\u001b[?25h"]}],"source":["!npm install localtunnel"]},{"cell_type":"markdown","source":["### Example"],"metadata":{"id":"wJzjG4lgaxCU"}},{"cell_type":"code","source":["import streamlit as st\n","\n","from langchain.agents import initialize_agent, AgentType\n","from langchain.callbacks import StreamlitCallbackHandler\n","from langchain.chat_models import ChatOpenAI\n","from langchain.tools import DuckDuckGoSearchRun\n","\n","with st.sidebar:\n","    openai_api_key = st.text_input(\"OpenAI API Key\", key=\"langchain_search_api_key_openai\", type=\"password\")\n","    \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n","    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/2_Chat_with_search.py)\"\n","    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)\"\n","\n","st.title(\"ðŸ”Ž LangChain - Chat with search\")\n","\n","\"\"\"\n","In this example, we're using `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an interactive Streamlit app.\n","Try more LangChain ðŸ¤ Streamlit Agent examples at [github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n","\"\"\"\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state[\"messages\"] = [\n","        {\"role\": \"assistant\", \"content\": \"Hi, I'm a chatbot who can search the web. How can I help you?\"}\n","    ]\n","\n","for msg in st.session_state.messages:\n","    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","if prompt := st.chat_input(placeholder=\"Who won the Women's U.S. Open in 2018?\"):\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    st.chat_message(\"user\").write(prompt)\n","\n","    if not openai_api_key:\n","        st.info(\"Please add your OpenAI API key to continue.\")\n","        st.stop()\n","\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, streaming=True)\n","    search = DuckDuckGoSearchRun(name=\"Search\")\n","    search_agent = initialize_agent([search], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, handle_parsing_errors=True)\n","    with st.chat_message(\"assistant\"):\n","        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)\n","        response = search_agent.run(st.session_state.messages, callbacks=[st_cb])\n","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n","        st.write(response)"],"metadata":{"id":"IvLAUQeHaEke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First\n","import openai import streamlit as st\n","with st.sidebar:\n","    openai_api_key = st.text_input(\"OpenAI API Key\", key=\"chatbot_api_key\", type=\"password\")\n","    \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n","    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)\"\n","    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)\"\n","\n","st.title(\"ðŸ’¬ Chatbot\") if \"messages\" not in st.session_state:\n","    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n","\n","for msg in st.session_state.messages:\n","    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","if prompt := st.chat_input():\n","    if not openai_api_key:\n","        st.info(\"Please add your OpenAI API key to continue.\")\n","        st.stop()\n","\n","    openai.api_key = openai_api_key\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    st.chat_message(\"user\").write(prompt)\n","    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=st.session_state.messages)\n","    msg = response.choices[0].message\n","    st.session_state.messages.append(msg)\n","    st.chat_message(\"assistant\").write(msg.content)"],"metadata":{"id":"ebym0X_paVju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Second\n","import streamlit as st import anthropic\n","with st.sidebar:\n","    anthropic_api_key = st.text_input(\"Anthropic API Key\", key=\"file_qa_api_key\", type=\"password\")\n","    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/1_File_Q%26A.py)\"\n","    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)\"\n","\n","st.title(\"ðŸ“ File Q&A with Anthropic\") uploaded_file = st.file_uploader(\"Upload an article\", type=(\"txt\", \"md\")) question = st.text_input(\n","    \"Ask something about the article\",\n","    placeholder=\"Can you give me a short summary?\",\n","    disabled=not uploaded_file,\n",")\n","if uploaded_file and question and not anthropic_api_key:\n","    st.info(\"Please add your Anthropic API key to continue.\")\n","\n","if uploaded_file and question and anthropic_api_key:\n","    article = uploaded_file.read().decode()\n","    prompt = f\"\"\"{anthropic.HUMAN_PROMPT} Here's an article:\\n\\n\n","    {article}\\n\\n\\n\\n{question}{anthropic.AI_PROMPT}\"\"\"\n","\n","    client = anthropic.Client(api_key=anthropic_api_key)\n","    response = client.completions.create(\n","        prompt=prompt,\n","        stop_sequences=[anthropic.HUMAN_PROMPT],\n","        model=\"claude-v1\", #\"claude-2\" for Claude 2 model\n","        max_tokens_to_sample=100,\n","    )\n","    st.write(\"### Answer\")\n","    st.write(response.completion)"],"metadata":{"id":"y7ZnxiQzaW-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run"],"metadata":{"id":"CQL0dtnba3ws"}},{"cell_type":"code","source":["%%writefile Chatbot.py\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    Conversation,\n",")\n","from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n","from langchain import PromptTemplate, LLMChain\n","import numpy as np\n","import pandas as pd\n","import torch\n","from datasets import Dataset, load_dataset\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","from sklearn.model_selection import train_test_split\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","cache_dir=\"./Model/\"\n","model_name=\"HuggingFaceH4/zephyr-7b-beta\"\n","\n","import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","class Chatbot():\n","  def __init__(self,model_name = \"HuggingFaceH4/zephyr-7b-beta\",cache_dir=\"./Model/\",max_vram = 7):\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    self.model_name = model_name\n","    self.cache_dir = cache_dir\n","    self.quantization_config = BitsAndBytesConfig( load_in_4bit=True,\n","                                                  bnb_4bit_compute_dtype=torch.bfloat16,\n","                                                  bnb_4bit_use_double_quant=True,\n","                                                  bnb_4bit_quant_type='nf4')\n","    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,cache_dir=self.cache_dir)\n","    self.tokenizer.pad_token = self.tokenizer.eos_token\n","    self.tokenizer.padding_side = \"right\"\n","    self.pipe=None\n","    self.langchain_hf=None\n","    self.max_vram = max_vram\n","\n","  def InitChat(self,lora=None):\n","    torch.cuda.empty_cache()\n","    self.model = AutoModelForCausalLM.from_pretrained( model_name,\n","                                                      cache_dir=self.cache_dir,\n","                                                      low_cpu_mem_usage=True,\n","                                                      return_dict=True,\n","                                                      quantization_config=self.quantization_config,\n","                                                      # if error -> use use_flash_attention_1\n","                                                      #use_flash_attention_2=True,\n","                                                      max_memory={0: f\"{self.max_vram}GB\"},\n","                                                      device_map=\"auto\",\n","                                                      torch_dtype=torch.bfloat16,\n","      )\n","    if lora != None:\n","      self.model = PeftModel.from_pretrained(self.model, lora)\n","      self.model = self.model.merge_and_unload()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer)\n","\n","  def InitLangChain(self,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer,\n","                         do_sample=True,max_new_tokens = max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p)\n","    self.langchain_hf = HuggingFacePipeline(pipeline=self.pipe)\n","  def CreatConversation(self):\n","    return  [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"You are a wonderful and trustworthy assistant. If you receive a request in Vietnamese, please respond in Vietnamese.\",\n","        }\n","    ]\n","  def __AddUserInput(self,content,messages):\n","     messages.append({\"role\": \"user\", \"content\": f\"{content.strip()}\"})\n","     return messages\n","\n","  def __AddAssistantOutput(self,content,messages):\n","    temp = (content.split(\"<|assistant|>\"))[-1].strip()\n","    messages.append({\"role\": \"assistant\", \"content\": f\"{temp}\"})\n","    return messages\n","\n","  def Chat(self,content:str =\"\",messages = None,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95):\n","    torch.cuda.empty_cache()\n","    if len(content)==0:\n","      return None\n","    if messages == None:\n","      messages = self.CreatConversation()\n","\n","    messages = self.__AddUserInput(content,messages)\n","    return self._Generate(messages,max_new_tokens,temperature, top_k, top_p)\n","\n","  def _Generate(self,messages,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    # use_flash_attention_1 if use_flash_attention_2 error\n","    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n","      prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","      outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p)\n","      messages = self.__AddAssistantOutput(outputs[0][\"generated_text\"],messages)\n","    return messages\n","\n","  def GetLastContent(self,messages):\n","    return (messages[-1][\"content\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SBy3iVn0iNP","executionInfo":{"status":"ok","timestamp":1702021743212,"user_tz":-420,"elapsed":15,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"}},"outputId":"e2adef3d-c3f4-4aae-f298-7dbdccaa88e1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting Chatbot.py\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","\n","from Chatbot import Chatbot\n","from langchain.agents import initialize_agent, AgentType\n","from langchain.callbacks import StreamlitCallbackHandler\n","from langchain.chat_models import ChatOpenAI\n","from langchain.tools import DuckDuckGoSearchRun\n","\n","@st.cache_resource\n","def init():\n","  chat = Chatbot(max_vram=13)\n","  chat.InitChat()\n","  return chat\n","\n","@st.cache_data\n","def chat_process(_chat,content,messages):\n","  return _chat.Chat(content,messages)\n","\n","def main():\n","  chat = init()\n","  with st.sidebar:\n","    st.title(\"My Chatbot\")\n","\n","  st.title(\"ðŸ’¬ Chat\")\n","\n","  if \"messages\" not in st.session_state:\n","      st.session_state[\"messages\"] = chat.CreatConversation()\n","      st.session_state.messages.append({\"role\": \"assistant\", \"content\": \"Xin chÃ o, TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?\"})\n","  for msg in st.session_state.messages:\n","    if msg[\"role\"] != \"system\":\n","      st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","  if prompt:= st.chat_input(placeholder=\"input message\"):\n","      # session_state.messages references chat messages\n","      # st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","      st.chat_message(\"user\").write(prompt)\n","      response = chat_process(chat,prompt,st.session_state.messages)\n","      msg = response[-1]\n","      # st.session_state.messages.append(msg)\n","      st.chat_message(\"assistant\").write(msg[\"content\"])\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"0AYHwdK3ac7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702021746806,"user_tz":-420,"elapsed":330,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"}},"outputId":"cc0724b1-24e2-4954-c49e-61625b056bab"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3PmMziu7u9e","outputId":"053db6cd-83c6-489c-b6db-d5113272c251","executionInfo":{"status":"ok","timestamp":1702022075625,"user_tz":-420,"elapsed":325713,"user":{"displayName":"Anh TrÃºc BÃ¹i","userId":"12513312395118645037"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Password/Enpoint IP for localtunnel is: 34.170.72.66\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.215s\n","your url is: https://few-papayas-lay.loca.lt\n","^C\n"]}],"source":["import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n","!streamlit run app.py &>logs.txt &\n","!npx localtunnel --port 8501"]}],"metadata":{"colab":{"collapsed_sections":["RVjF_bP_fsp3","wJzjG4lgaxCU"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4e0469bec19a4932bd326f097fdc4877":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6eeb77a98251458b939fdf77a51f6fb6","IPY_MODEL_530f8864945d4c629f8c79c532976904","IPY_MODEL_49c2095c4d5046a9aef6f4bc0731fd68"],"layout":"IPY_MODEL_7bcde2193c794db2956d14d30eb9a1a4"}},"6eeb77a98251458b939fdf77a51f6fb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac6e0af92824eb7b30596ab3eac24a0","placeholder":"â€‹","style":"IPY_MODEL_3e4ce89f991b44e284177d24c8514753","value":"Loading checkpoint shards: 100%"}},"530f8864945d4c629f8c79c532976904":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb1934c7bb564828a63203119b5a7572","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8c44e79eb954271bc1f89f3a64fab94","value":8}},"49c2095c4d5046a9aef6f4bc0731fd68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc815aea32644f40a4bf4c4155d82b09","placeholder":"â€‹","style":"IPY_MODEL_79a81d43658c4e7f8842233526fedfca","value":" 8/8 [05:40&lt;00:00, 39.00s/it]"}},"7bcde2193c794db2956d14d30eb9a1a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac6e0af92824eb7b30596ab3eac24a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e4ce89f991b44e284177d24c8514753":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb1934c7bb564828a63203119b5a7572":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c44e79eb954271bc1f89f3a64fab94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc815aea32644f40a4bf4c4155d82b09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79a81d43658c4e7f8842233526fedfca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}