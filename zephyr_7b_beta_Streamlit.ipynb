{"cells":[{"cell_type":"markdown","metadata":{"id":"X1BtiuAOfHfX"},"source":["## Install"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":108655,"status":"ok","timestamp":1702018253452,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"},"user_tz":-420},"id":"xPhy5XqEd_KU"},"outputs":[],"source":["from IPython.display import clear_output\n","#install\n","!pip install -U transformers\n","!pip install -U huggingface-hub\n","!pip install -U langchain\n","!pip install -U accelerate\n","!pip install -U datasets\n","\n","# not exit in python 3.11\n","#!pip install -U intel_extension_for_pytorch\n","\n","!pip install -U bitsandbytes\n","# use in windows machine. !!!May not be safe!!!\n","#!pip install -U bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n","!pip install -U peft\n","!pip install -U trl\n","!pip install -U pprintpp\n","\n","\n","# install FlashAttention 2\n","# !pip install -U ninja\n","# !pip install -U packaging\n","# !MAX_JOBS=4 pip install -U flash-attn --no-build-isolation # if error -> use pip install -U flash-attn\n","\n","!pip install -U streamlit\n","!pip install -U langchain-experimental\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"6C_lztdCQST6"},"source":["## Colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3268,"status":"ok","timestamp":1702018256711,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"},"user_tz":-420},"id":"J79KlPRYQEXH","outputId":"3543f940-3188-4340-869a-3defaf1d6128"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1702018256712,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"},"user_tz":-420},"id":"TCNNlfzyQRq_","outputId":"1c13c003-393f-4b2d-9748-7434619189d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/zephyr\n"]}],"source":["%cd \"/content/drive/MyDrive/Colab Notebooks/zephyr\""]},{"cell_type":"markdown","metadata":{"id":"CXAps0JlfPBr"},"source":["## Import"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":341,"status":"ok","timestamp":1702021716805,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"},"user_tz":-420},"id":"w6YGVVK8uAWZ"},"outputs":[],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    Conversation,\n",")\n","from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n","from langchain import PromptTemplate, LLMChain\n","import numpy as np\n","import pandas as pd\n","import torch\n","from datasets import Dataset, load_dataset\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","from sklearn.model_selection import train_test_split\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","cache_dir=\"./Model/\"\n","model_name=\"HuggingFaceH4/zephyr-7b-beta\"\n","\n","import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n"]},{"cell_type":"markdown","metadata":{"id":"ogSZ1QMLMk1g"},"source":["## Model"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"4WU9a0FTpMxv","executionInfo":{"status":"ok","timestamp":1702021718582,"user_tz":-420,"elapsed":479,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"}}},"outputs":[],"source":["class Chatbot():\n","  def __init__(self,model_name = \"HuggingFaceH4/zephyr-7b-beta\",cache_dir=\"./Model/\",max_vram = 7):\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    self.model_name = model_name\n","    self.cache_dir = cache_dir\n","    self.quantization_config = BitsAndBytesConfig( load_in_4bit=True,\n","                                                  bnb_4bit_compute_dtype=torch.bfloat16,\n","                                                  bnb_4bit_use_double_quant=True,\n","                                                  bnb_4bit_quant_type='nf4')\n","    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,cache_dir=self.cache_dir)\n","    self.tokenizer.pad_token = self.tokenizer.eos_token\n","    self.tokenizer.padding_side = \"right\"\n","    self.pipe=None\n","    self.langchain_hf=None\n","    self.max_vram = max_vram\n","\n","  def InitChat(self,lora=None):\n","    torch.cuda.empty_cache()\n","    self.model = AutoModelForCausalLM.from_pretrained( model_name,\n","                                                      cache_dir=self.cache_dir,\n","                                                      low_cpu_mem_usage=True,\n","                                                      return_dict=True,\n","                                                      quantization_config=self.quantization_config,\n","                                                      # if error -> use use_flash_attention_1\n","                                                      #use_flash_attention_2=True,\n","                                                      max_memory={0: f\"{self.max_vram}GB\"},\n","                                                      device_map=\"auto\",\n","                                                      torch_dtype=torch.bfloat16,\n","      )\n","    if lora != None:\n","      self.model = PeftModel.from_pretrained(self.model, lora)\n","      self.model = self.model.merge_and_unload()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer)\n","\n","  def InitLangChain(self,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer,\n","                         do_sample=True,max_new_tokens = max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p)\n","    self.langchain_hf = HuggingFacePipeline(pipeline=self.pipe)\n","  def CreatConversation(self):\n","    return  [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"You are a wonderful and trustworthy assistant. If you receive a request in Vietnamese, please respond in Vietnamese.\",\n","        }\n","    ]\n","  def __AddUserInput(self,content,messages):\n","     messages.append({\"role\": \"user\", \"content\": f\"{content.strip()}\"})\n","     return messages\n","\n","  def __AddAssistantOutput(self,content,messages):\n","    temp = (content.split(\"<|assistant|>\"))[-1].strip()\n","    messages.append({\"role\": \"assistant\", \"content\": f\"{temp}\"})\n","    return messages\n","\n","  def Chat(self,content:str =\"\",messages = None,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95):\n","    torch.cuda.empty_cache()\n","    if len(content)==0:\n","      return None\n","    if messages == None:\n","      messages = self.CreatConversation()\n","\n","    messages = self.__AddUserInput(content,messages)\n","    return self._Generate(messages,max_new_tokens,temperature, top_k, top_p)\n","\n","  def _Generate(self,messages,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    # use_flash_attention_1 if use_flash_attention_2 error\n","    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n","      prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","      outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p)\n","      messages = self.__AddAssistantOutput(outputs[0][\"generated_text\"],messages)\n","    return messages\n","\n","  def GetLastContent(self,messages):\n","    return (messages[-1][\"content\"])"]},{"cell_type":"markdown","metadata":{"id":"RVjF_bP_fsp3"},"source":["## Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["a3c9cdf96f244523afa15a7c20ba58f1"]},"id":"rJ0YSvTBgK0e","outputId":"37e3eceb-7935-4329-f49d-f56223aa6b64"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3c9cdf96f244523afa15a7c20ba58f1","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["chat = Chatbot()\n","chat.InitChat()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10520,"status":"ok","timestamp":1701676140142,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"xUu-rhOSaCRA","outputId":"67d7859b-560c-4810-a784-893eccbc2e7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Xin chào, tôi cá nhận rất vui với việc giúp bạn. Vui lòng để lại yêu cầu của bạn ở đây, tôi sẽ trả lời ngay.\n"]}],"source":["messages = chat.Chat(\"Xin chào tôi cần giúp đỡ\",max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTdkGqAC2K7t","outputId":"f4e067c3-4284-4488-eecd-76d081f2e034"},"outputs":[{"name":"stdout","output_type":"stream","text":["Một năm có 365 ngày (trừ năm nhuận đơn) trong khoảng giữa hai ngày của tháng 12 và tháng 0 (các môi trường khác có thể sử dụng hệ thống năm khác như 360 ngày trong một năm).\n"]}],"source":["messages = chat.Chat(\"Một năm có bao nhiêu ngày\",messages,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33562,"status":"ok","timestamp":1701676179893,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"hHX2DuUja7um","outputId":"4cf8dc54-b56f-41a1-b460-a3c8a66dc4f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Độc quyền (copyright) là quyền tác giả hoặc tác giả có quyền sở hữu được đưa ra khi tạo ra một tập lược, hình ảnh, khám phá, động thái, văn bản, hoặc không gian bản quyền khác. Nó cho phép chủ sở hữu có quyền tự do để chọn các cách sử dụng, phân phối, sản xuất, hoặc bán của bản quyền đó. Độc quyền cũng có thể được truyền qua không gian bản quyền sau khi chủ sở hữu chết, nhưng thời gian nà có thể khác tùy thể sự nghiệp của chủ sở hữu.\n"]}],"source":["messages = chat.Chat(\"Gải thích thuật ngữ \\\"độc quyền\\\"\",messages,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11708,"status":"ok","timestamp":1701676191598,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"ZgypQ26BbGaa","outputId":"41498ca2-4b14-4d11-ccbb-3d2ac88b0895"},"outputs":[{"name":"stdout","output_type":"stream","text":["Xin hãy không heesitate nếu có bất kỳ thắc mắc nà khác, tôi sẽ cố gắng cung cấp các thông tin chính xác nhất có sẵn.\n"]}],"source":["messages = chat.Chat(\"Cảm ơn thông tin rất hữu ích\",messages,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95)\n","print(chat.GetLastContent(messages))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1701676191598,"user":{"displayName":"Hayato Kishima","userId":"05741786304760218572"},"user_tz":-420},"id":"re4gHEvqsn3a","outputId":"cf53920c-bf09-4845-e793-09e94159f372"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'role': 'system', 'content': 'You are a wonderful and trustworthy assistant. If you receive a request in Vietnamese, please respond in Vietnamese.'}, {'role': 'user', 'content': 'Xin chào tôi cần giúp đỡ'}, {'role': 'assistant', 'content': 'Xin chào, tôi cá nhận rất vui với việc giúp bạn. Vui lòng để lại yêu cầu của bạn ở đây, tôi sẽ trả lời ngay.'}, {'role': 'user', 'content': 'Một năm có bao nhiêu ngày'}, {'role': 'assistant', 'content': 'Một năm có 365 ngày (trừ năm nhuận đơn) trong khoảng giữa hai ngày của tháng 12 và tháng 0 (các môi trường khác có thể sử dụng hệ thống năm khác như 360 ngày trong một năm).'}, {'role': 'user', 'content': 'Gải thích thuật ngữ \"độc quyền\"'}, {'role': 'assistant', 'content': 'Độc quyền (copyright) là quyền tác giả hoặc tác giả có quyền sở hữu được đưa ra khi tạo ra một tập lược, hình ảnh, khám phá, động thái, văn bản, hoặc không gian bản quyền khác. Nó cho phép chủ sở hữu có quyền tự do để chọn các cách sử dụng, phân phối, sản xuất, hoặc bán của bản quyền đó. Độc quyền cũng có thể được truyền qua không gian bản quyền sau khi chủ sở hữu chết, nhưng thời gian nà có thể khác tùy thể sự nghiệp của chủ sở hữu.'}, {'role': 'user', 'content': 'Cảm ơn thông tin rất hữu ích'}, {'role': 'assistant', 'content': 'Xin hãy không heesitate nếu có bất kỳ thắc mắc nà khác, tôi sẽ cố gắng cung cấp các thông tin chính xác nhất có sẵn.'}]\n"]}],"source":["print(messages)"]},{"cell_type":"markdown","metadata":{"id":"YPum52jHOKQN"},"source":["### Using LangChain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4e0469bec19a4932bd326f097fdc4877","6eeb77a98251458b939fdf77a51f6fb6","530f8864945d4c629f8c79c532976904","49c2095c4d5046a9aef6f4bc0731fd68","7bcde2193c794db2956d14d30eb9a1a4","fac6e0af92824eb7b30596ab3eac24a0","3e4ce89f991b44e284177d24c8514753","bb1934c7bb564828a63203119b5a7572","d8c44e79eb954271bc1f89f3a64fab94","fc815aea32644f40a4bf4c4155d82b09","79a81d43658c4e7f8842233526fedfca"]},"executionInfo":{"elapsed":357436,"status":"ok","timestamp":1701933943156,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"},"user_tz":-420},"id":"imzmu9VpONQh","outputId":"73bd7c88-649f-494b-cff4-e9c6c7d42dda"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0469bec19a4932bd326f097fdc4877"}},"metadata":{}}],"source":["chat = Chatbot(max_vram=13)\n","chat.InitLangChain(max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95)"]},{"cell_type":"markdown","source":["### Pandas"],"metadata":{"id":"S5fXnzIPOyFa"}},{"cell_type":"code","source":["from langchain.agents.agent_types import AgentType\n","from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n","import pandas as pd\n","\n","\n","data = pd.read_excel(\"Data.xlsx\")\n","\n","agent = create_pandas_dataframe_agent(chat.langchain_hf, data, verbose=True)\n","\n","agent.run(\"Có bao nhiêu ngành học ở trường khoa học máy tính\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"xN9bI6tZLQaC","executionInfo":{"status":"ok","timestamp":1701935711640,"user_tz":-420,"elapsed":737421,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"}},"outputId":"0262ed4e-03c7-482c-abc4-3c9af14ce8d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32;1m\u001b[1;3mThought: Tìm kiếm từ khoa học máy tính trong bảng\n","Action: python_repl_ast\n","Action Input: len(df[(df['content'].str.contains('Khoa học máy tính'))])\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m1\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m Được biết có 1 ngành học ở trường khoa học máy tính\n","Final Answer: 1\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"H_C_qPBS7sZI"},"source":["## Streamlit"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4595,"status":"ok","timestamp":1702021743212,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"},"user_tz":-420},"id":"PRMxqtt-82ms","outputId":"aac433ef-dfa9-4aaa-a129-b10c6ab9b09b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/drive/MyDrive/Colab Notebooks/zephyr/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/drive/MyDrive/Colab Notebooks/zephyr/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m zephyr No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","updated 1 package and audited 36 packages in 3.561s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n","  run `npm audit fix` to fix them, or `npm audit` for details\n","\u001b[K\u001b[?25h"]}],"source":["!npm install localtunnel"]},{"cell_type":"markdown","source":["### Example"],"metadata":{"id":"wJzjG4lgaxCU"}},{"cell_type":"code","source":["import streamlit as st\n","\n","from langchain.agents import initialize_agent, AgentType\n","from langchain.callbacks import StreamlitCallbackHandler\n","from langchain.chat_models import ChatOpenAI\n","from langchain.tools import DuckDuckGoSearchRun\n","\n","with st.sidebar:\n","    openai_api_key = st.text_input(\"OpenAI API Key\", key=\"langchain_search_api_key_openai\", type=\"password\")\n","    \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n","    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/2_Chat_with_search.py)\"\n","    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)\"\n","\n","st.title(\"🔎 LangChain - Chat with search\")\n","\n","\"\"\"\n","In this example, we're using `StreamlitCallbackHandler` to display the thoughts and actions of an agent in an interactive Streamlit app.\n","Try more LangChain 🤝 Streamlit Agent examples at [github.com/langchain-ai/streamlit-agent](https://github.com/langchain-ai/streamlit-agent).\n","\"\"\"\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state[\"messages\"] = [\n","        {\"role\": \"assistant\", \"content\": \"Hi, I'm a chatbot who can search the web. How can I help you?\"}\n","    ]\n","\n","for msg in st.session_state.messages:\n","    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","if prompt := st.chat_input(placeholder=\"Who won the Women's U.S. Open in 2018?\"):\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    st.chat_message(\"user\").write(prompt)\n","\n","    if not openai_api_key:\n","        st.info(\"Please add your OpenAI API key to continue.\")\n","        st.stop()\n","\n","    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, streaming=True)\n","    search = DuckDuckGoSearchRun(name=\"Search\")\n","    search_agent = initialize_agent([search], llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, handle_parsing_errors=True)\n","    with st.chat_message(\"assistant\"):\n","        st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)\n","        response = search_agent.run(st.session_state.messages, callbacks=[st_cb])\n","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n","        st.write(response)"],"metadata":{"id":"IvLAUQeHaEke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First\n","import openai import streamlit as st\n","with st.sidebar:\n","    openai_api_key = st.text_input(\"OpenAI API Key\", key=\"chatbot_api_key\", type=\"password\")\n","    \"[Get an OpenAI API key](https://platform.openai.com/account/api-keys)\"\n","    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)\"\n","    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)\"\n","\n","st.title(\"💬 Chatbot\") if \"messages\" not in st.session_state:\n","    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n","\n","for msg in st.session_state.messages:\n","    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","if prompt := st.chat_input():\n","    if not openai_api_key:\n","        st.info(\"Please add your OpenAI API key to continue.\")\n","        st.stop()\n","\n","    openai.api_key = openai_api_key\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    st.chat_message(\"user\").write(prompt)\n","    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=st.session_state.messages)\n","    msg = response.choices[0].message\n","    st.session_state.messages.append(msg)\n","    st.chat_message(\"assistant\").write(msg.content)"],"metadata":{"id":"ebym0X_paVju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Second\n","import streamlit as st import anthropic\n","with st.sidebar:\n","    anthropic_api_key = st.text_input(\"Anthropic API Key\", key=\"file_qa_api_key\", type=\"password\")\n","    \"[View the source code](https://github.com/streamlit/llm-examples/blob/main/pages/1_File_Q%26A.py)\"\n","    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)\"\n","\n","st.title(\"📝 File Q&A with Anthropic\") uploaded_file = st.file_uploader(\"Upload an article\", type=(\"txt\", \"md\")) question = st.text_input(\n","    \"Ask something about the article\",\n","    placeholder=\"Can you give me a short summary?\",\n","    disabled=not uploaded_file,\n",")\n","if uploaded_file and question and not anthropic_api_key:\n","    st.info(\"Please add your Anthropic API key to continue.\")\n","\n","if uploaded_file and question and anthropic_api_key:\n","    article = uploaded_file.read().decode()\n","    prompt = f\"\"\"{anthropic.HUMAN_PROMPT} Here's an article:\\n\\n\n","    {article}\\n\\n\\n\\n{question}{anthropic.AI_PROMPT}\"\"\"\n","\n","    client = anthropic.Client(api_key=anthropic_api_key)\n","    response = client.completions.create(\n","        prompt=prompt,\n","        stop_sequences=[anthropic.HUMAN_PROMPT],\n","        model=\"claude-v1\", #\"claude-2\" for Claude 2 model\n","        max_tokens_to_sample=100,\n","    )\n","    st.write(\"### Answer\")\n","    st.write(response.completion)"],"metadata":{"id":"y7ZnxiQzaW-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run"],"metadata":{"id":"CQL0dtnba3ws"}},{"cell_type":"code","source":["%%writefile Chatbot.py\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    Conversation,\n",")\n","from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n","from langchain import PromptTemplate, LLMChain\n","import numpy as np\n","import pandas as pd\n","import torch\n","from datasets import Dataset, load_dataset\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","from sklearn.model_selection import train_test_split\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","cache_dir=\"./Model/\"\n","model_name=\"HuggingFaceH4/zephyr-7b-beta\"\n","\n","import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","class Chatbot():\n","  def __init__(self,model_name = \"HuggingFaceH4/zephyr-7b-beta\",cache_dir=\"./Model/\",max_vram = 7):\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    self.model_name = model_name\n","    self.cache_dir = cache_dir\n","    self.quantization_config = BitsAndBytesConfig( load_in_4bit=True,\n","                                                  bnb_4bit_compute_dtype=torch.bfloat16,\n","                                                  bnb_4bit_use_double_quant=True,\n","                                                  bnb_4bit_quant_type='nf4')\n","    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,cache_dir=self.cache_dir)\n","    self.tokenizer.pad_token = self.tokenizer.eos_token\n","    self.tokenizer.padding_side = \"right\"\n","    self.pipe=None\n","    self.langchain_hf=None\n","    self.max_vram = max_vram\n","\n","  def InitChat(self,lora=None):\n","    torch.cuda.empty_cache()\n","    self.model = AutoModelForCausalLM.from_pretrained( model_name,\n","                                                      cache_dir=self.cache_dir,\n","                                                      low_cpu_mem_usage=True,\n","                                                      return_dict=True,\n","                                                      quantization_config=self.quantization_config,\n","                                                      # if error -> use use_flash_attention_1\n","                                                      #use_flash_attention_2=True,\n","                                                      max_memory={0: f\"{self.max_vram}GB\"},\n","                                                      device_map=\"auto\",\n","                                                      torch_dtype=torch.bfloat16,\n","      )\n","    if lora != None:\n","      self.model = PeftModel.from_pretrained(self.model, lora)\n","      self.model = self.model.merge_and_unload()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer)\n","\n","  def InitLangChain(self,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    self.pipe = pipeline(\"text-generation\", model=self.model,tokenizer=self.tokenizer,\n","                         do_sample=True,max_new_tokens = max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p)\n","    self.langchain_hf = HuggingFacePipeline(pipeline=self.pipe)\n","  def CreatConversation(self):\n","    return  [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"You are a wonderful and trustworthy assistant. If you receive a request in Vietnamese, please respond in Vietnamese.\",\n","        }\n","    ]\n","  def __AddUserInput(self,content,messages):\n","     messages.append({\"role\": \"user\", \"content\": f\"{content.strip()}\"})\n","     return messages\n","\n","  def __AddAssistantOutput(self,content,messages):\n","    temp = (content.split(\"<|assistant|>\"))[-1].strip()\n","    messages.append({\"role\": \"assistant\", \"content\": f\"{temp}\"})\n","    return messages\n","\n","  def Chat(self,content:str =\"\",messages = None,max_new_tokens = 4096,temperature=0.6, top_k=50, top_p=0.95):\n","    torch.cuda.empty_cache()\n","    if len(content)==0:\n","      return None\n","    if messages == None:\n","      messages = self.CreatConversation()\n","\n","    messages = self.__AddUserInput(content,messages)\n","    return self._Generate(messages,max_new_tokens,temperature, top_k, top_p)\n","\n","  def _Generate(self,messages,max_new_tokens = 4096, temperature=0.6, top_k=50, top_p=0.95):\n","    if self.pipe==None:\n","      self.InitChat()\n","    # use_flash_attention_1 if use_flash_attention_2 error\n","    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n","      prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","      outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=top_k, top_p=top_p)\n","      messages = self.__AddAssistantOutput(outputs[0][\"generated_text\"],messages)\n","    return messages\n","\n","  def GetLastContent(self,messages):\n","    return (messages[-1][\"content\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SBy3iVn0iNP","executionInfo":{"status":"ok","timestamp":1702021743212,"user_tz":-420,"elapsed":15,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"}},"outputId":"e2adef3d-c3f4-4aae-f298-7dbdccaa88e1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting Chatbot.py\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","\n","from Chatbot import Chatbot\n","from langchain.agents import initialize_agent, AgentType\n","from langchain.callbacks import StreamlitCallbackHandler\n","from langchain.chat_models import ChatOpenAI\n","from langchain.tools import DuckDuckGoSearchRun\n","\n","@st.cache_resource\n","def init():\n","  chat = Chatbot(max_vram=13)\n","  chat.InitChat()\n","  return chat\n","\n","@st.cache_data\n","def chat_process(_chat,content,messages):\n","  return _chat.Chat(content,messages)\n","\n","def main():\n","  chat = init()\n","  with st.sidebar:\n","    st.title(\"My Chatbot\")\n","\n","  st.title(\"💬 Chat\")\n","\n","  if \"messages\" not in st.session_state:\n","      st.session_state[\"messages\"] = chat.CreatConversation()\n","      st.session_state.messages.append({\"role\": \"assistant\", \"content\": \"Xin chào, Tôi có thể giúp gì cho bạn?\"})\n","  for msg in st.session_state.messages:\n","    if msg[\"role\"] != \"system\":\n","      st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n","\n","  if prompt:= st.chat_input(placeholder=\"input message\"):\n","      # session_state.messages references chat messages\n","      # st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","      st.chat_message(\"user\").write(prompt)\n","      response = chat_process(chat,prompt,st.session_state.messages)\n","      msg = response[-1]\n","      # st.session_state.messages.append(msg)\n","      st.chat_message(\"assistant\").write(msg[\"content\"])\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"0AYHwdK3ac7J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702021746806,"user_tz":-420,"elapsed":330,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"}},"outputId":"cc0724b1-24e2-4954-c49e-61625b056bab"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3PmMziu7u9e","outputId":"053db6cd-83c6-489c-b6db-d5113272c251","executionInfo":{"status":"ok","timestamp":1702022075625,"user_tz":-420,"elapsed":325713,"user":{"displayName":"Anh Trúc Bùi","userId":"12513312395118645037"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Password/Enpoint IP for localtunnel is: 34.170.72.66\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.215s\n","your url is: https://few-papayas-lay.loca.lt\n","^C\n"]}],"source":["import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n","!streamlit run app.py &>logs.txt &\n","!npx localtunnel --port 8501"]}],"metadata":{"colab":{"collapsed_sections":["RVjF_bP_fsp3","wJzjG4lgaxCU"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4e0469bec19a4932bd326f097fdc4877":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6eeb77a98251458b939fdf77a51f6fb6","IPY_MODEL_530f8864945d4c629f8c79c532976904","IPY_MODEL_49c2095c4d5046a9aef6f4bc0731fd68"],"layout":"IPY_MODEL_7bcde2193c794db2956d14d30eb9a1a4"}},"6eeb77a98251458b939fdf77a51f6fb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fac6e0af92824eb7b30596ab3eac24a0","placeholder":"​","style":"IPY_MODEL_3e4ce89f991b44e284177d24c8514753","value":"Loading checkpoint shards: 100%"}},"530f8864945d4c629f8c79c532976904":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb1934c7bb564828a63203119b5a7572","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8c44e79eb954271bc1f89f3a64fab94","value":8}},"49c2095c4d5046a9aef6f4bc0731fd68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc815aea32644f40a4bf4c4155d82b09","placeholder":"​","style":"IPY_MODEL_79a81d43658c4e7f8842233526fedfca","value":" 8/8 [05:40&lt;00:00, 39.00s/it]"}},"7bcde2193c794db2956d14d30eb9a1a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac6e0af92824eb7b30596ab3eac24a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e4ce89f991b44e284177d24c8514753":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb1934c7bb564828a63203119b5a7572":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8c44e79eb954271bc1f89f3a64fab94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc815aea32644f40a4bf4c4155d82b09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79a81d43658c4e7f8842233526fedfca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}